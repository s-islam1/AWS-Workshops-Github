{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Lab 2: Personalize our agent by adding memory\n",
    "\n",
    "### Overview\n",
    "\n",
    "In Lab 1, you built a Customer Support Agent that worked well for a single user in a local session. However, real-world customer support needs to scale beyond a single user running in a local environment.\n",
    "\n",
    "When we run an **Agent in Production**, we'll need:\n",
    "- **Multi-User Support**: Handle thousands of customers simultaneously\n",
    "- **Persistent Storage**: Save conversations beyond session lifecycle\n",
    "- **Long-Term Learning**: Extract customer preferences and behavioral patterns\n",
    "- **Cross-Session Continuity**: Remember customers across different interactions\n",
    "\n",
    "**Workshop Progress:**\n",
    "- **Lab 1 (Done)**: Create Agent Prototype - Build a functional customer support agent\n",
    "- **Lab 2 (Current)**: Enhance with Memory - Add conversation context and personalization\n",
    "- **Lab 3**: Scale with Gateway & Identity - Share tools across agents securely\n",
    "- **Lab 4**: Deploy to Production - Use AgentCore Runtime with observability\n",
    "- **Lab 5**: Build User Interface - Create a customer-facing application\n",
    "\n",
    "\n",
    "In this lab, you'll add the missing persistence and learning layer that transforms your Goldfish-Agent (forgets the conversation in seconds) into an smart personalized Assistant.\n",
    "\n",
    "Memory is a critical component of intelligence. While Large Language Models (LLMs) have impressive capabilities, they lack persistent memory across conversations. [Amazon Bedrock AgentCore Memory](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory-getting-started.html) addresses this limitation by providing a managed service that enables AI agents to maintain context over time, remember important facts, and deliver consistent, personalized experiences.\n",
    "\n",
    "AgentCore Memory operates on two levels:\n",
    "- **Short-Term Memory**: Immediate conversation context and session-based information that provides continuity within a single interaction or closely related sessions.\n",
    "- **Long-Term Memory**: Persistent information extracted and stored across multiple conversations, including facts, preferences, and summaries that enable personalized experiences over time.\n",
    "\n",
    "### Architecture for Lab 2\n",
    "<div style=\"text-align:left\">\n",
    "    <img src=\"images/architecture_lab2_memory.png\" width=\"75%\"/>\n",
    "</div>\n",
    "\n",
    "*Multi-user agent with persistent short term and long term memory capabilities. *\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* **AWS Account** with appropriate permissions\n",
    "* **Python 3.10+** installed locally\n",
    "* **AWS CLI configured** with credentials\n",
    "* **Anthropic Claude 3.7** enabled on [Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html)\n",
    "* **Strands Agents** and other libraries installed in the next cells\n",
    "* These resources are created for you within an AWS workshop account\n",
    "    - AWS Lambda function \n",
    "    - AWS Lambda Execution IAM Role\n",
    "    - AgentCore Gateway IAM Role\n",
    "    - DynamoDB tables used by the AWS Lambda function. \n",
    "    - Cognito User Pool and User Pool Client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "\n",
    "Let's import the libraries for AgentCore Memory. For it, we will use the [Amazon Bedrock AgentCore Python SDK](https://github.com/aws/bedrock-agentcore-sdk-python), a lightweight wrapper that helps you working with AgentCore capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:21:23.935857Z",
     "iopub.status.busy": "2025-10-24T18:21:23.935599Z",
     "iopub.status.idle": "2025-10-24T18:21:24.589780Z",
     "shell.execute_reply": "2025-10-24T18:21:24.589243Z",
     "shell.execute_reply.started": "2025-10-24T18:21:23.935835Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from boto3.session import Session\n",
    "\n",
    "\n",
    "# Import AgentCore Memory client and Hook events from strands\n",
    "from bedrock_agentcore.memory import MemoryClient\n",
    "from bedrock_agentcore.memory.constants import StrategyType\n",
    "from strands.hooks import (\n",
    "    AfterInvocationEvent,\n",
    "    HookProvider,\n",
    "    HookRegistry,\n",
    "    MessageAddedEvent,\n",
    ")\n",
    "\n",
    "from lab_helpers.utils import get_ssm_parameter, put_ssm_parameter\n",
    "\n",
    "boto_session = Session()\n",
    "REGION = boto_session.region_name\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Bedrock AgentCore Memory resources\n",
    "\n",
    "Amazon Bedrock AgentCore Memory is a fully managed service that provides persistent memory capabilities for AI agents.\n",
    "\n",
    "#### AgentCore Memory Concepts:\n",
    "\n",
    "1. **Short-Term Memory (STM)**: Immediately stores conversation context within the session\n",
    "2. **Long-Term Memory (LTM)**: Asynchronously processes STM to extract meaningful patterns, preferences and facts\n",
    "3. **Memory Strategies**: Different approaches for extracting and organizing information:\n",
    "   - **USER_PREFERENCE**: Learns customer preferences, behaviors, and patterns\n",
    "   - **SEMANTIC**: Stores factual information using vector embeddings for similarity search\n",
    "4. **Namespaces**: Logical grouping of memories by customer and context type. We'll create these two namespaces:\n",
    "- `support/customer/{actorId}/preferences`: Customer preferences and behavioral patterns\n",
    "- `support/customer/{actorId}/semantic`: Factual information and conversation history\n",
    "\n",
    "This structure enables multi-tenant memory where each customer's information is isolated and easily retrievable.\n",
    "\n",
    "#### Memory Creation Process:\n",
    "\n",
    "Creating memory resources involves provisioning the underlying infrastructure (vector databases, processing pipelines, etc.). This typically takes 2-3 minutes as AWS sets up the managed services behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:21:28.986421Z",
     "iopub.status.busy": "2025-10-24T18:21:28.986045Z",
     "iopub.status.idle": "2025-10-24T18:21:29.005358Z",
     "shell.execute_reply": "2025-10-24T18:21:29.004856Z",
     "shell.execute_reply.started": "2025-10-24T18:21:28.986396Z"
    }
   },
   "outputs": [],
   "source": [
    "memory_client = MemoryClient(region_name=REGION)\n",
    "memory_name = \"CustomerSupportMemory\"\n",
    "\n",
    "\n",
    "def create_or_get_memory_resource():\n",
    "    try:\n",
    "        memory_id = get_ssm_parameter(\"/app/customersupport/agentcore/memory_id\")\n",
    "        memory_client.gmcp_client.get_memory(memoryId=memory_id)\n",
    "        return memory_id\n",
    "    except Exception:\n",
    "        try:\n",
    "            strategies = [\n",
    "                {\n",
    "                    StrategyType.USER_PREFERENCE.value: {\n",
    "                        \"name\": \"CustomerPreferences\",\n",
    "                        \"description\": \"Captures customer preferences and behavior\",\n",
    "                        \"namespaces\": [\"support/customer/{actorId}/preferences\"],\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    StrategyType.SEMANTIC.value: {\n",
    "                        \"name\": \"CustomerSupportSemantic\",\n",
    "                        \"description\": \"Stores facts from conversations\",\n",
    "                        \"namespaces\": [\"support/customer/{actorId}/semantic\"],\n",
    "                    }\n",
    "                },\n",
    "            ]\n",
    "            print(\"Creating AgentCore Memory resources. This will take 2-3 minutes...\")\n",
    "            print(\"While we wait, let's understand what's happening behind the scenes:\")\n",
    "            print(\"• Setting up managed vector databases for semantic search\")\n",
    "            print(\"• Configuring memory extraction pipelines\")\n",
    "            print(\"• Provisioning secure, multi-tenant storage\")\n",
    "            print(\"• Establishing namespace isolation for customer data\")\n",
    "            # *** AGENTCORE MEMORY USAGE *** - Create memory resource with semantic strategy\n",
    "            response = memory_client.create_memory_and_wait(\n",
    "                name=memory_name,\n",
    "                description=\"Customer support agent memory\",\n",
    "                strategies=strategies,\n",
    "                event_expiry_days=90,  # Memories expire after 90 days\n",
    "            )\n",
    "            memory_id = response[\"id\"]\n",
    "            try:\n",
    "                put_ssm_parameter(\"/app/customersupport/agentcore/memory_id\", memory_id)\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "            return memory_id\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create memory resource: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:21:32.261005Z",
     "iopub.status.busy": "2025-10-24T18:21:32.260756Z",
     "iopub.status.idle": "2025-10-24T18:24:26.260863Z",
     "shell.execute_reply": "2025-10-24T18:24:26.260348Z",
     "shell.execute_reply.started": "2025-10-24T18:21:32.260986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating AgentCore Memory resources. This will take 2-3 minutes...\n",
      "While we wait, let's understand what's happening behind the scenes:\n",
      "• Setting up managed vector databases for semantic search\n",
      "• Configuring memory extraction pipelines\n",
      "• Provisioning secure, multi-tenant storage\n",
      "• Establishing namespace isolation for customer data\n",
      "✅ AgentCore Memory created successfully!\n",
      "Memory ID: CustomerSupportMemory-LY7ZAXChAk\n"
     ]
    }
   ],
   "source": [
    "memory_id = create_or_get_memory_resource()\n",
    "if memory_id:\n",
    "    print(\"✅ AgentCore Memory created successfully!\")\n",
    "    print(f\"Memory ID: {memory_id}\")\n",
    "else:\n",
    "    print(\"Memory resource not created. Try Again !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Seed previous customer interactions\n",
    "\n",
    "**Why are we seeding memory?**\n",
    "\n",
    "In production, agents accumulate memory naturally through customer interactions. However, for this lab, we're seeding historical conversations to demonstrate how Long-Term Memory (LTM) works without waiting for real conversations.\n",
    "\n",
    "**How memory processing works:**\n",
    "1. `create_event` stores interactions in **Short-Term Memory** (STM) instantly\n",
    "2. STM is asynchronously processed by **Long-Term Memory** strategies\n",
    "3. LTM extracts patterns, preferences, and facts for future retrieval\n",
    "\n",
    "Let's seed some customer history to see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:24:26.261856Z",
     "iopub.status.busy": "2025-10-24T18:24:26.261648Z",
     "iopub.status.idle": "2025-10-24T18:24:26.577615Z",
     "shell.execute_reply": "2025-10-24T18:24:26.577093Z",
     "shell.execute_reply.started": "2025-10-24T18:24:26.261839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Arn: arn:aws:bedrock-agentcore:us-west-2:146859427221:memory/CustomerSupportMemory-LY7ZAXChAk\n",
      "Memory ID: CustomerSupportMemory-LY7ZAXChAk\n",
      "--------------------------------------------------------------------\n",
      "✅ Seeded customer history successfully\n",
      "📝 Interactions saved to Short-Term Memory\n",
      "⏳ Long-Term Memory processing will begin automatically...\n"
     ]
    }
   ],
   "source": [
    "# List existing memory resources\n",
    "for memory in memory_client.list_memories():\n",
    "    print(f\"Memory Arn: {memory.get('arn')}\")\n",
    "    print(f\"Memory ID: {memory.get('id')}\")\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "\n",
    "# Seed with previous customer interactions\n",
    "CUSTOMER_ID = \"customer_001\"\n",
    "\n",
    "previous_interactions = [\n",
    "    (\"I'm having issues with my MacBook Pro overheating during video editing.\", \"USER\"),\n",
    "    (\n",
    "        \"I can help with that thermal issue. For video editing workloads, let's check your Activity Monitor and adjust performance settings. Your MacBook Pro order #MB-78432 is still under warranty.\",\n",
    "        \"ASSISTANT\",\n",
    "    ),\n",
    "    (\n",
    "        \"What's the return policy on gaming headphones? I need low latency for competitive FPS games\",\n",
    "        \"USER\",\n",
    "    ),\n",
    "    (\n",
    "        \"For gaming headphones, you have 30 days to return. Since you're into competitive FPS, I'd recommend checking the audio latency specs - most gaming models have <40ms latency.\",\n",
    "        \"ASSISTANT\",\n",
    "    ),\n",
    "    (\n",
    "        \"I need a laptop under $1200 for programming. Prefer 16GB RAM minimum and good Linux compatibility. I like ThinkPad models.\",\n",
    "        \"USER\",\n",
    "    ),\n",
    "    (\n",
    "        \"Perfect! For development work, I'd suggest looking at our ThinkPad E series or Dell XPS models. Both have excellent Linux support and 16GB RAM options within your budget.\",\n",
    "        \"ASSISTANT\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Save previous interactions\n",
    "if memory_id:\n",
    "    try:\n",
    "        memory_client.create_event(\n",
    "            memory_id=memory_id,\n",
    "            actor_id=CUSTOMER_ID,\n",
    "            session_id=\"previous_session\",\n",
    "            messages=previous_interactions,\n",
    "        )\n",
    "        print(\"✅ Seeded customer history successfully\")\n",
    "        print(\"📝 Interactions saved to Short-Term Memory\")\n",
    "        print(\"⏳ Long-Term Memory processing will begin automatically...\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error seeding history: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Memory Processing\n",
    "\n",
    "After creating events with `create_event`, AgentCore Memory processes the data in two stages:\n",
    "\n",
    "1. **Immediate**: Messages stored in Short-Term Memory (STM)\n",
    "2. **Asynchronous**: STM processed into Long-Term Memory (LTM) strategies\n",
    "\n",
    "LTM processing typically takes 20-30 seconds as the system:\n",
    "- Analyzes conversation patterns\n",
    "- Extracts customer preferences and behaviors\n",
    "- Creates semantic embeddings for factual information\n",
    "- Organizes memories by namespace for efficient retrieval\n",
    "\n",
    "Let's check if our Long-Term Memory processing is complete by retrieving customer preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:24:26.578292Z",
     "iopub.status.busy": "2025-10-24T18:24:26.578105Z",
     "iopub.status.idle": "2025-10-24T18:25:07.679399Z",
     "shell.execute_reply": "2025-10-24T18:25:07.678904Z",
     "shell.execute_reply.started": "2025-10-24T18:24:26.578274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking for processed Long-Term Memories...\n",
      "⏳ Still processing... waiting 10 more seconds (attempt 1/6)\n",
      "⏳ Still processing... waiting 10 more seconds (attempt 2/6)\n",
      "⏳ Still processing... waiting 10 more seconds (attempt 3/6)\n",
      "⏳ Still processing... waiting 10 more seconds (attempt 4/6)\n",
      "✅ Found 3 preference memories after 40 seconds!\n",
      "🎯 AgentCore Memory automatically extracted these customer preferences from our seeded conversations:\n",
      "================================================================================\n",
      "  1. {\"context\":\"The user explicitly stated they need good Linux compatibility.\",\"preference\":\"Prefers laptops with good Linux compatibility\",\"categories\":[\"technology\",\"computers\",\"operating systems\"]}\n",
      "  2. {\"context\":\"The user mentioned needing low latency for gaming headphones.\",\"preference\":\"Prefers low latency gaming headphones\",\"categories\":[\"gaming peripherals\",\"audio equipment\"]}\n",
      "  3. {\"context\":\"The user mentioned needing low latency headphones for competitive FPS games.\",\"preference\":\"Plays competitive FPS games\",\"categories\":[\"gaming\",\"entertainment\"]}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Wait for Long-Term Memory processing to complete\n",
    "print(\"🔍 Checking for processed Long-Term Memories...\")\n",
    "retries = 0\n",
    "max_retries = 6  # 1 minute wait\n",
    "\n",
    "while retries < max_retries:\n",
    "    memories = memory_client.retrieve_memories(\n",
    "        memory_id=memory_id,\n",
    "        namespace=f\"support/customer/{CUSTOMER_ID}/preferences\",\n",
    "        query=\"can you summarize the support issue\",\n",
    "    )\n",
    "\n",
    "    if memories:\n",
    "        print(\n",
    "            f\"✅ Found {len(memories)} preference memories after {retries * 10} seconds!\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "    retries += 1\n",
    "    if retries < max_retries:\n",
    "        print(\n",
    "            f\"⏳ Still processing... waiting 10 more seconds (attempt {retries}/{max_retries})\"\n",
    "        )\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        print(\n",
    "            \"⚠️ Memory processing is taking longer than expected. This can happen with overloading..\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "print(\n",
    "    \"🎯 AgentCore Memory automatically extracted these customer preferences from our seeded conversations:\"\n",
    ")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, memory in enumerate(memories, 1):\n",
    "    if isinstance(memory, dict):\n",
    "        content = memory.get(\"content\", {})\n",
    "        if isinstance(content, dict):\n",
    "            text = content.get(\"text\", \"\")\n",
    "            print(f\"  {i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Semantic Memory\n",
    "\n",
    "Semantic memory stores factual information from conversations using vector embeddings. This enables similarity-based retrieval of relevant facts and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:25:07.680450Z",
     "iopub.status.busy": "2025-10-24T18:25:07.680261Z",
     "iopub.status.idle": "2025-10-24T18:25:07.839139Z",
     "shell.execute_reply": "2025-10-24T18:25:07.838623Z",
     "shell.execute_reply.started": "2025-10-24T18:25:07.680432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 AgentCore Memory identified these factual details from conversations:\n",
      "================================================================================\n",
      "  1. The user is looking for a laptop under $1200 for programming.\n",
      "  2. The user plays competitive FPS games and needs low latency gaming headphones.\n",
      "  3. The user has a MacBook Pro that is overheating during video editing.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Retrieve semantic memories (factual information)\n",
    "while True:\n",
    "    semantic_memories = memory_client.retrieve_memories(\n",
    "        memory_id=memory_id,\n",
    "        namespace=f\"support/customer/{CUSTOMER_ID}/semantic\",\n",
    "        query=\"information on the technical support issue\",\n",
    "    )\n",
    "    print(\"🧠 AgentCore Memory identified these factual details from conversations:\")\n",
    "    print(\"=\" * 80)\n",
    "    if memories:\n",
    "        break\n",
    "    time.sleep(10)\n",
    "for i, memory in enumerate(semantic_memories, 1):\n",
    "    if isinstance(memory, dict):\n",
    "        content = memory.get(\"content\", {})\n",
    "        if isinstance(content, dict):\n",
    "            text = content.get(\"text\", \"\")\n",
    "            print(f\"  {i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Implement Strands Hooks to save and retrieve agent interactions\n",
    "\n",
    "Now we'll integrate AgentCore Memory with our agent using Strands' hook system. This creates an automatic memory layer that works seamlessly with any agent conversation.\n",
    "\n",
    "- **MessageAddedEvent**: Triggered when messages are added to the conversation, allowing us to retrieve and inject customer context\n",
    "- **AfterInvocationEvent**: Fired after agent responses, enabling automatic storage of interactions to memory\n",
    "\n",
    "The hook system ensures memory operations happen automatically without manual intervention, creating a seamless experience where customer context is preserved across conversations.\n",
    "\n",
    "To create the hooks we will extend the `HookProvider` class:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:25:07.839832Z",
     "iopub.status.busy": "2025-10-24T18:25:07.839648Z",
     "iopub.status.idle": "2025-10-24T18:25:07.848549Z",
     "shell.execute_reply": "2025-10-24T18:25:07.848110Z",
     "shell.execute_reply.started": "2025-10-24T18:25:07.839815Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomerSupportMemoryHooks(HookProvider):\n",
    "    \"\"\"Memory hooks for customer support agent\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, memory_id: str, client: MemoryClient, actor_id: str, session_id: str\n",
    "    ):\n",
    "        self.memory_id = memory_id\n",
    "        self.client = client\n",
    "        self.actor_id = actor_id\n",
    "        self.session_id = session_id\n",
    "        self.namespaces = {\n",
    "            i[\"type\"]: i[\"namespaces\"][0]\n",
    "            for i in self.client.get_memory_strategies(self.memory_id)\n",
    "        }\n",
    "\n",
    "    def retrieve_customer_context(self, event: MessageAddedEvent):\n",
    "        \"\"\"Retrieve customer context before processing support query\"\"\"\n",
    "        messages = event.agent.messages\n",
    "        if (\n",
    "            messages[-1][\"role\"] == \"user\"\n",
    "            and \"toolResult\" not in messages[-1][\"content\"][0]\n",
    "        ):\n",
    "            user_query = messages[-1][\"content\"][0][\"text\"]\n",
    "\n",
    "            try:\n",
    "                all_context = []\n",
    "\n",
    "                for context_type, namespace in self.namespaces.items():\n",
    "                    # *** AGENTCORE MEMORY USAGE *** - Retrieve customer context from each namespace\n",
    "                    memories = self.client.retrieve_memories(\n",
    "                        memory_id=self.memory_id,\n",
    "                        namespace=namespace.format(actorId=self.actor_id),\n",
    "                        query=user_query,\n",
    "                        top_k=3,\n",
    "                    )\n",
    "                    # Post-processing: Format memories into context strings\n",
    "                    for memory in memories:\n",
    "                        if isinstance(memory, dict):\n",
    "                            content = memory.get(\"content\", {})\n",
    "                            if isinstance(content, dict):\n",
    "                                text = content.get(\"text\", \"\").strip()\n",
    "                                if text:\n",
    "                                    all_context.append(\n",
    "                                        f\"[{context_type.upper()}] {text}\"\n",
    "                                    )\n",
    "\n",
    "                # Inject customer context into the query\n",
    "                if all_context:\n",
    "                    context_text = \"\\n\".join(all_context)\n",
    "                    original_text = messages[-1][\"content\"][0][\"text\"]\n",
    "                    messages[-1][\"content\"][0][\"text\"] = (\n",
    "                        f\"Customer Context:\\n{context_text}\\n\\n{original_text}\"\n",
    "                    )\n",
    "                    logger.info(f\"Retrieved {len(all_context)} customer context items\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to retrieve customer context: {e}\")\n",
    "\n",
    "    def save_support_interaction(self, event: AfterInvocationEvent):\n",
    "        \"\"\"Save customer support interaction after agent response\"\"\"\n",
    "        try:\n",
    "            messages = event.agent.messages\n",
    "            if len(messages) >= 2 and messages[-1][\"role\"] == \"assistant\":\n",
    "                # Get last customer query and agent response\n",
    "                customer_query = None\n",
    "                agent_response = None\n",
    "\n",
    "                for msg in reversed(messages):\n",
    "                    if msg[\"role\"] == \"assistant\" and not agent_response:\n",
    "                        agent_response = msg[\"content\"][0][\"text\"]\n",
    "                    elif (\n",
    "                        msg[\"role\"] == \"user\"\n",
    "                        and not customer_query\n",
    "                        and \"toolResult\" not in msg[\"content\"][0]\n",
    "                    ):\n",
    "                        customer_query = msg[\"content\"][0][\"text\"]\n",
    "                        break\n",
    "\n",
    "                if customer_query and agent_response:\n",
    "                    # *** AGENTCORE MEMORY USAGE *** - Save the support interaction\n",
    "                    self.client.create_event(\n",
    "                        memory_id=self.memory_id,\n",
    "                        actor_id=self.actor_id,\n",
    "                        session_id=self.session_id,\n",
    "                        messages=[\n",
    "                            (customer_query, \"USER\"),\n",
    "                            (agent_response, \"ASSISTANT\"),\n",
    "                        ],\n",
    "                    )\n",
    "                    logger.info(\"Saved support interaction to memory\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save support interaction: {e}\")\n",
    "\n",
    "    def register_hooks(self, registry: HookRegistry) -> None:\n",
    "        \"\"\"Register customer support memory hooks\"\"\"\n",
    "        registry.add_callback(MessageAddedEvent, self.retrieve_customer_context)\n",
    "        registry.add_callback(AfterInvocationEvent, self.save_support_interaction)\n",
    "        logger.info(\"Customer support memory hooks registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create a Customer Support Agent with memory\n",
    "\n",
    "Next, we will implement the Customer Support Agent just as we did in Lab 1, but this time we instantiate the class `CustomerSupportMemoryHooks` and we pass the memory hook to the agent contructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:25:07.849160Z",
     "iopub.status.busy": "2025-10-24T18:25:07.848990Z",
     "iopub.status.idle": "2025-10-24T18:25:08.222455Z",
     "shell.execute_reply": "2025-10-24T18:25:08.221944Z",
     "shell.execute_reply.started": "2025-10-24T18:25:07.849143Z"
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "\n",
    "from lab_helpers.lab1_strands_agent import (\n",
    "    SYSTEM_PROMPT,\n",
    "    get_return_policy,\n",
    "    web_search,\n",
    "    get_product_info,\n",
    "    get_technical_support,\n",
    "    MODEL_ID,\n",
    ")\n",
    "\n",
    "SESSION_ID = str(uuid.uuid4())\n",
    "memory_hooks = CustomerSupportMemoryHooks(\n",
    "    memory_id, memory_client, CUSTOMER_ID, SESSION_ID\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the Bedrock model (Anthropic Claude 3.7 Sonnet)\n",
    "model = BedrockModel(model_id=MODEL_ID, region_name=REGION)\n",
    "\n",
    "# Create the customer support agent with all 5 tools\n",
    "agent = Agent(\n",
    "    model=model,\n",
    "    hooks=[memory_hooks],  # Pass Memory Hooks\n",
    "    tools=[\n",
    "        get_product_info,  # Tool 1: Simple product information lookup\n",
    "        get_return_policy,  # Tool 2: Simple return policy lookup\n",
    "        web_search,\n",
    "        get_technical_support,\n",
    "    ],\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Personalized Agent\n",
    "\n",
    "Let's test our memory-enhanced agent! Watch how it uses the customer's historical preferences to provide personalized recommendations.\n",
    "\n",
    "The agent will automatically:\n",
    "1. Retrieve relevant customer context from memory\n",
    "2. Use that context to personalize the response\n",
    "3. Save this new interaction for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:25:08.223269Z",
     "iopub.status.busy": "2025-10-24T18:25:08.222984Z",
     "iopub.status.idle": "2025-10-24T18:25:35.537357Z",
     "shell.execute_reply": "2025-10-24T18:25:35.536774Z",
     "shell.execute_reply.started": "2025-10-24T18:25:08.223251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎧 Testing headphone recommendation with customer memory...\n",
      "\n",
      "\n",
      "I'd be happy to help you find gaming headphones that would meet your needs for competitive FPS gaming with low latency! Let me get some information about headphones for you.\n",
      "Tool #1: get_product_info\n",
      "Based on your preferences for competitive FPS gaming, I understand that low latency is critical for your gaming experience. Let me search for specific information about gaming headphones with low latency features to give you the best recommendations.\n",
      "Tool #2: web_search\n",
      "I apologize, but my search didn't return specific gaming headphone recommendations. Let me try a more focused search to find the best low latency gaming headphones for competitive FPS games.\n",
      "Tool #3: web_search\n",
      "I'm not getting the specific gaming headphone information I'm looking for. Let's try one more focused search to get you the most accurate recommendations.\n",
      "Tool #4: web_search\n",
      "Based on the information I've gathered, I can now provide some recommendations for low latency headphones that would be ideal for your competitive FPS gaming needs.\n",
      "\n",
      "### Recommendations for Low Latency Gaming Headphones:\n",
      "\n",
      "#### For the Absolute Lowest Latency:\n",
      "**Wired Gaming Headsets**\n",
      "Wired headphones are still the gold standard for competitive FPS gaming as they offer zero latency. These would be my top recommendation if minimizing delay is your absolute priority.\n",
      "\n",
      "#### For Wireless Options with Minimal Latency:\n",
      "1. **Headsets with 2.4GHz Wireless Technology**: These offer much lower latency than Bluetooth connections, typically in the range of 20-30ms, which is imperceptible to most gamers.\n",
      "\n",
      "2. **Ranger HX500** (based on search results): Features ultra-low 28ms latency with dual 2.4GHz wireless connectivity, Bluetooth, and wired options. The detachable boom mic ensures clear communication with teammates, and the battery lasts up to 25 hours.\n",
      "\n",
      "3. **Logitech with Lightspeed Wireless Technology**: Logitech's Lightspeed wireless connection maintains ultrafast, low-latency audio performance that's crucial for competitive gaming.\n",
      "\n",
      "### Key Features to Look For:\n",
      "1. **Connection Type**: Wired for zero latency or 2.4GHz wireless for minimal delay\n",
      "2. **Audio Quality**: Good spatial audio to help pinpoint enemy positions in FPS games\n",
      "3. **Comfort**: For extended gaming sessions\n",
      "4. **Microphone Quality**: Clear communication with teammates\n",
      "5. **Battery Life**: For wireless options, look for 20+ hours of battery life\n",
      "\n",
      "### Additional Considerations:\n",
      "- Some headsets offer specific FPS gaming modes that enhance footstep sounds and other crucial audio cues\n",
      "- Cross-platform compatibility if you play on multiple systems\n",
      "- Software support for audio customization\n",
      "\n",
      "Would you like more specific information about any of these headset options? Or are there particular features that are most important to you besides low latency that I should consider when making recommendations?"
     ]
    }
   ],
   "source": [
    "print(\"🎧 Testing headphone recommendation with customer memory...\\n\\n\")\n",
    "response1 = agent(\"Which headphones would you recommend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T18:25:35.538147Z",
     "iopub.status.busy": "2025-10-24T18:25:35.537947Z",
     "iopub.status.idle": "2025-10-24T18:25:42.303237Z",
     "shell.execute_reply": "2025-10-24T18:25:42.302739Z",
     "shell.execute_reply.started": "2025-10-24T18:25:35.538129Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💻 Testing laptop preference recall...\n",
      "\n",
      "\n",
      "Based on your customer context, I can see your preferences for laptops. Let me summarize your preferred laptop brand and requirements:\n",
      "\n",
      "### Your Laptop Preferences:\n",
      "\n",
      "- **Preferred Brand**: ThinkPad models\n",
      "- **Primary Use**: Programming\n",
      "- **RAM Requirement**: At least 16GB RAM\n",
      "- **Operating System Compatibility**: Good Linux compatibility\n",
      "- **Budget**: Under $1,200\n",
      "- **Other Considerations**: You seem to be looking for a reliable development machine that works well with Linux\n",
      "\n",
      "ThinkPad laptops are indeed excellent choices for programming and Linux compatibility. Many developers prefer ThinkPad models because they typically offer good hardware support for Linux distributions, have durable build quality, and feature comfortable keyboards for long coding sessions.\n",
      "\n",
      "Would you like me to provide some specific ThinkPad model recommendations that meet your requirements for programming, with at least 16GB RAM, good Linux compatibility, and under $1,200? Or is there anything else about your laptop preferences you'd like to discuss?"
     ]
    }
   ],
   "source": [
    "print(\"\\n💻 Testing laptop preference recall...\\n\\n\")\n",
    "response2 = agent(\"What is my preferred laptop brand and requirements?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the Agent remembers:\n",
    "• Your gaming preferences (low latency headphones)\n",
    "• Your laptop preferences (ThinkPad, 16GB RAM, Linux compatibility)\n",
    "• Your budget constraints ($1200 for laptops)\n",
    "• Previous technical issues (MacBook overheating)\n",
    "\n",
    "This is the power of AgentCore Memory - persistent, personalized customer experiences!\n",
    "\n",
    "## Congratulations! 🎉\n",
    "\n",
    "You have successfully completed **Lab 2: Add memory to the Customer Support Agent**!\n",
    "\n",
    "### What You Accomplished:\n",
    "\n",
    "- Created a serverless managed memory with Amazon Bedrock AgentCore Memory\n",
    "- Implemented long-term memory to store User-Preferences and Semantic (Factual) information.\n",
    "- Integrated AgentCore Memory with the customer support Agent using the hook mechanism provided by Strands Agents\n",
    "\n",
    "##### Next Up [Lab 3 - Scaling with Gateway and Identity  →](lab-03-agentcore-gateway.ipynb)\n",
    "\n",
    "## Resources\n",
    "- [Amazon Bedrock Agent Core Memory](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/memory.html)\n",
    "- [Amazon Bedrock AgentCore Memory Deep Dive blog](https://aws.amazon.com/blogs/machine-learning/amazon-bedrock-agentcore-memory-building-context-aware-agents/)\n",
    "- [Strands Agents Hooks Documentation](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/agents/hooks/?h=hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
